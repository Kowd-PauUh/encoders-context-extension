# Zero-Training Context Extension for Transformer Encoders via Positional Embeddings Approximation
Official implementation of "Zero-Training Context Extension for Transformer Encoders via Positional Embeddings Approximation". Paper preprint is coming soon.

## Models

Models are available at HuggingFace:

|Model|Context length|Language|
|-|-|-|
|[idanylenko/e5-large-v2-ctx1024](https://huggingface.co/idanylenko/e5-large-v2-ctx1024)|1024|English|
